import google.generativeai as genai
genai.configure(api_key="AIzaSyAO2yhWsZA5fk3H9-N4BMoH3c9oz4PQSnE")

import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import google.generativeai as genai

# Configure Gemini API key (thay b·∫±ng key th·∫≠t c·ªßa b·∫°n t·ª´ Google AI Studio)
genai.configure(api_key="AIzaSyBzrosbf3CCJlm9AIuGbApVit4yUPcWLQQ")

# ================== C·∫§U H√åNH ==================
JSON_FILE = "/content/drive/RAG/all_procedures_normalized.json"  # ƒê∆∞·ªùng d·∫´n file JSON (sau chunk rule-based)
CHROMA_DB_PATH = "chroma_db"  # Th∆∞ m·ª•c l∆∞u vector DB
COLLECTION_NAME = "dichvucong_rag"
GEMINI_MODEL = "gemini-2.5-flash"  # Ho·∫∑c "gemini-1.5-pro"

@st.cache_resource
def get_embedding_function():
    EMBEDDING_MODEL = "BAAI/bge-m3"  # Model embedding ti·∫øng Vi·ªát
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL)
    return embedding_function

@st.cache_resource
def load_collection():
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    embedding_func = get_embedding_function()

    try:
        collection = chroma_client.get_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_func  # c·∫ßn ƒë·ªÉ query ƒë√∫ng
        )
        #st.success(f"Collection '{COLLECTION_NAME}' ƒë√£ load t·ª´ {CHROMA_DB_PATH}")
    except Exception as e:
        st.error(f"Kh√¥ng t√¨m th·∫•y collection '{COLLECTION_NAME}' trong {CHROMA_DB_PATH}: {e}")
        collection = None

    return collection
# --- Load collection 1 l·∫ßn ---
collection = load_collection()

def query_rag(query: str, chat_history: list, top_k: int):
    # Retrieval v·ªõi top_k ƒë·ªông
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    context_parts = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        context_parts.append(f"[{meta['hierarchy']}]\\n{doc}\\n(Ngu·ªìn: {meta['url']})")

    context = "\\n\\n".join(context_parts)

    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam.
B·∫°n ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi.
KH√îNG ƒë∆∞·ª£c vi·∫øt l·∫°i, di·ªÖn ƒë·∫°t l·∫°i hay s·ª≠a ƒë·ªïi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
KH√îNG nh·∫Øc l·∫°i c√¢u h·ªèi.
PH·∫†M VI √ÅP D·ª§NG:
- ∆Øu ti√™n t∆∞ v·∫•n c√°c th·ªß t·ª•c h√†nh ch√≠nh li√™n quan ƒë·∫øn tr·∫ª em d∆∞·ªõi 6 tu·ªïi.
- N·∫øu CONTEXT kh√¥ng ƒë·ªÅ c·∫≠p r√µ ƒë·ªô tu·ªïi nh∆∞ng n·ªôi dung thu·ªôc th·ªß t·ª•c th∆∞·ªùng √°p d·ª•ng cho tr·∫ª em,
  b·∫°n ƒë∆∞·ª£c ph√©p tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin hi·ªán c√≥ v√† n√™u r√µ ph·∫°m vi √°p d·ª•ng n·∫øu ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p.

NGUY√äN T·∫ÆC TR·∫¢ L·ªúI:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong CONTEXT b√™n d∆∞·ªõi.
- Kh√¥ng s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i.
- Kh√¥ng t·ª± b·ªï sung th√¥ng tin kh√¥ng c√≥ trong CONTEXT.
- Kh√¥ng t·ª± thay ƒë·ªïi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

C√ÅCH TR·∫¢ L·ªúI:
- Ch·ªâ tr·∫£ l·ªùi c√°c n·ªôi dung LI√äN QUAN TR·ª∞C TI·∫æP ƒë·∫øn c√¢u h·ªèi.
- C√≥ th·ªÉ t·ªïng h·ª£p nhi·ªÅu ƒëo·∫°n trong CONTEXT n·∫øu ch√∫ng c√πng m√¥ t·∫£ m·ªôt th·ªß t·ª•c.
- Tr√¨nh b√†y ng·∫Øn g·ªçn, r√µ r√†ng, ƒë√∫ng tr·ªçng t√¢m.

TR∆Ø·ªúNG H·ª¢P KH√îNG TR·∫¢ L·ªúI:
Ch·ªâ tr·∫£ l·ªùi ƒë√∫ng c√¢u sau n·∫øu:
- CONTEXT ho√†n to√†n kh√¥ng ch·ª©a th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi.

C√¢u tr·∫£ l·ªùi trong tr∆∞·ªùng h·ª£p n√†y PH·∫¢I CH√çNH X√ÅC:
"Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong d·ªØ li·ªáu hi·ªán c√≥."

Y√äU C·∫¶U ƒê·ªäNH D·∫†NG:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
- N·∫øu c√≥ nhi·ªÅu √Ω, tr√¨nh b√†y b·∫±ng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c ƒë√°nh s·ªë.
- Gi·ªØ nguy√™n tr√≠ch d·∫´n ngu·ªìn n·∫øu c√≥ trong CONTEXT.

    Context:
    {context}

    C√¢u h·ªèi: {query}

    Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, c√≥ ƒë√°nh s·ªë n·∫øu l√† danh s√°ch, v√† tr√≠ch d·∫´n ngu·ªìn r√µ r√†ng (t√™n block, URL):
    """

    model = genai.GenerativeModel(GEMINI_MODEL)
    response = model.generate_content(prompt, stream=True)

    return response

# ================== GIAO DI·ªÜN CH√çNH ==================
st.set_page_config(
    page_title="Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh tr·∫ª em d∆∞·ªõi 6 tu·ªïi",
    page_icon="ü§ñ",
    layout="centered"
)

# ================== TI√äU ƒê·ªÄ ==================
st.title("ü§ñ Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh tr·∫ª em d∆∞·ªõi 6 tu·ªïi")
st.markdown(
    "H·ªó tr·ª£ t∆∞ v·∫•n **ƒëƒÉng k√Ω khai sinh ‚Äì ƒëƒÉng k√Ω th∆∞·ªùng tr√∫ ‚Äì c·∫•p th·∫ª BHYT** "
    "cho **tr·∫ª em d∆∞·ªõi 6 tu·ªïi** d·ª±a tr√™n d·ªØ li·ªáu ch√≠nh th·ªëng."
)

# ================== SIDEBAR ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

with st.sidebar:
    st.markdown("## üìú L·ªãch s·ª≠ tr√≤ chuy·ªán")

    if st.session_state.messages:
        for i, msg in enumerate(st.session_state.messages):
            if msg["role"] == "user":
                st.markdown(f"**üë§ Ng∆∞·ªùi d√πng:** {msg['content']}")
            else:
                st.markdown(f"**ü§ñ Chatbot:** {msg['content'][:150]}...")
            st.divider()
    else:
        st.caption("Ch∆∞a c√≥ cu·ªôc tr√≤ chuy·ªán n√†o.")

    if collection:
        try:
           data = collection.get(include=["metadatas"])
           metadatas = data.get("metadatas", [])

           source_files = set()

           for meta in metadatas:
              if not meta:
                continue

              file_name = meta.get("source_file", "").strip()
              if file_name:
                source_files.add(file_name)

        except Exception as e:
            st.error(f"L·ªói khi t·∫£i file d·ªØ li·ªáu: {e}")
    else:
        st.caption("Ch∆∞a t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu vector.")

    if collection:
        metadatas = collection.get(
            include=["metadatas"],
            limit=10
        )["metadatas"]

        seen = set()
        for meta in metadatas:
            url = meta.get("url", "")
            code = meta.get("procedure_code", "")
            if url and url not in seen:
                st.markdown(f"- **{code}**: [Link]({url})")
                seen.add(url)
    else:
        st.caption("Ch∆∞a t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.")

    st.divider()

    st.markdown("## ‚ÑπÔ∏è Th√¥ng tin h·ªá th·ªëng")
    st.write(f"üì¶ Vector DB: {COLLECTION_NAME}")
    st.write(f"üß© S·ªë chunk: {collection.count() if collection else 0}")
    st.write(f"ü§ñ LLM: {GEMINI_MODEL}")
    st.write("üìê Embedding: BAAI/bge-m3")
    st.caption("D·ªØ li·ªáu ƒë∆∞·ª£c load t·ª´ file JSON.")

# ================== KH·ªûI T·∫†O L·ªäCH S·ª¨ CHAT ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

# ================== HI·ªÇN TH·ªä L·ªäCH S·ª¨ CHAT ==================
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ================== INPUT T·ª™ USER ==================
prompt = st.chat_input(
    "H·ªèi v·ªÅ th·ªß t·ª•c h√†nh ch√≠nh cho tr·∫ª em d∆∞·ªõi 6 tu·ªïi "
    "(v√≠ d·ª•: Gi·∫•y khai sinh c√≥ c·∫•p b·∫£n ƒëi·ªán t·ª≠ kh√¥ng?)"
)

if prompt:
    # L∆∞u c√¢u h·ªèi
    st.session_state.messages.append(
        {"role": "user", "content": prompt}
    )

    with st.chat_message("user"):
        st.markdown(prompt)

    # ================== G·ªåI BACKEND (GI·ªÆ NGUY√äN) ==================
    with st.chat_message("assistant"):
        full_response = ""
        message_placeholder = st.empty()

        try:
            response = query_rag(prompt, st.session_state.messages, top_k)
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response)
            message_placeholder.markdown(full_response)
        except Exception as e:
            full_response = f"L·ªói khi g·ªçi Gemini: {str(e)}"
            message_placeholder.error(full_response)


    # L∆∞u c√¢u tr·∫£ l·ªùi
    st.session_state.messages.append(
        {"role": "assistant", "content": full_response}
    )
